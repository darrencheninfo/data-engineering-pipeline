{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student: Darren Chen\n",
    "# ADS 507 \n",
    "# Assignment 5.1  - SQL Query Performance Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel python 3.10.12 \n",
    "# Install the mysql-connector-python package\n",
    "# %pip install mysql\n",
    "# %pip install mysql-connector-python\n",
    "\n",
    "import mysql.connector \n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Azure Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure SQL Server Connection \n",
    "conn = mysql.connector.connect(\n",
    "    host=\"mysqldchen.mysql.database.azure.com\",\n",
    "    user=\"dchenAdmin\",\n",
    "    port=\"3306\",\n",
    "    password=\"507password!\",\n",
    "    database=\"cdcyrbss\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_query function \n",
    "def run_query(query):\n",
    "    # try: (indent next for error handling)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    for row in results:\n",
    "        print(row)\n",
    "    # except mysql.connector.Error as err:\n",
    "    #     print(f\"Error: {err}\")\n",
    "    # finally:\n",
    "    #     if conn.is_connected():\n",
    "    #         cursor.close()\n",
    "    #         conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ADSS data to sql datab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL DATA: Alcohol and Drug Services Study (ADSS), 1996-1999: [United States] (ICPSR 3088)\n",
    "LINK: https://www.icpsr.umich.edu/web/NAHDAP/studies/3088/export\n",
    "DOWNLOAD DELIMITED FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert TSV to CSV\n",
    "\n",
    "# Define the file paths \n",
    "tsv_file_path = r'C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0003\\03088-0003-Data.tsv'\n",
    "csv_file_path = r'C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0003\\03088-0003-Data.csv'\n",
    "\n",
    "# Open the TSV file for reading and the CSV file for writing\n",
    "with open(tsv_file_path, 'r', newline='', encoding='utf-8') as tsv_file, \\\n",
    "     open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    \n",
    "    # Create a TSV reader and a CSV writer\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "\n",
    "    # Iterate over each row in the TSV file and write it to the CSV file\n",
    "    for row in tsv_reader:\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(f\"TSV file has been successfully converted to CSV and saved at: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert tsv file to csv for import to mysql database: \n",
    "\n",
    "in VSCode Terminal enter the following to convert to csv:  \n",
    ">Import-Csv -Delimiter \"`t\" -Path \"C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0004\\03088-0004-Data.tsv\" | Export-Csv -NoTypeInformation -Path \"C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0004\\03088-0004-Data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICPSR_03088  Alcohol and Drug Services Study (ADSS)\n",
    "\n",
    "Part 3 - Phase 2 - Main Incentive Abstract data table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'group' has been removed.\n",
      "Updated file saved as: C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0003\\03088-0003-Data_updated.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0003\\\\03088-0003-Data.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part3 = pd.read_csv(csv_file)\n",
    "\n",
    "# Check if 'group' column exists before dropping\n",
    "if 'group' in df_part3.columns:\n",
    "    df_part3 = df_part3.drop(columns=['group'])\n",
    "    print(\"Column 'group' has been removed.\")\n",
    "else:\n",
    "    print(\"Column 'group' not found in the dataset.\")\n",
    "\n",
    "# Save updated DataFrame to a new CSV file\n",
    "output_file = csv_file.replace(\".csv\", \"_updated.csv\")\n",
    "df_part3.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated file saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into MySQL database.\n"
     ]
    }
   ],
   "source": [
    "# import mysql.connector\n",
    "# import pandas as pd\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    \"host\": \"mysqldchen.mysql.database.azure.com\",\n",
    "    \"user\": \"dchenAdmin\",\n",
    "    \"port\": 3306,\n",
    "    \"password\": \"507password!\",\n",
    "    \"database\": \"icpsr_03088\"\n",
    "}\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Generate table name\n",
    "table_name = \"adss_data_part3\"\n",
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0003\\\\03088-0003-Data_updated.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part3 = pd.read_csv(csv_file)\n",
    "\n",
    "# Get column names and data types\n",
    "columns = df_part3.columns\n",
    "data_types = df_part3.dtypes\n",
    "\n",
    "# Mapping pandas dtypes to MySQL column types\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"INT\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Define the table schema with caseid and facid as PRIMARY KEYS\n",
    "create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "for col, dtype in zip(columns, data_types):\n",
    "    mysql_type = dtype_mapping.get(str(dtype), \"TEXT\")  # Default to TEXT if unknown\n",
    "    create_table_query += f\"`{col}` {mysql_type}, \"\n",
    "\n",
    "# Define composite primary key (caseid, facid)\n",
    "create_table_query += \"PRIMARY KEY (`caseid`, `facid`));\"\n",
    "\n",
    "# Execute the table creation query\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "insert_query = f\"INSERT IGNORE INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(columns))})\"\n",
    "# iteratively adds rows to the table\n",
    "for _, row in df_part3.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully loaded into MySQL database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICPSR_03088  Alcohol and Drug Services Study (ADSS)\n",
    "\n",
    "Part 4 - Phase 2 - In-Treatment Methadone Abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'group' has been removed.\n",
      "Updated file saved as: C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0004\\03088-0004-Data_updated.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0004\\\\03088-0004-Data.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part4 = pd.read_csv(csv_file)\n",
    "\n",
    "# Check if 'group' column exists before dropping\n",
    "if 'group' in df_part4.columns:\n",
    "    df_part4 = df_part4.drop(columns=['group'])\n",
    "    print(\"Column 'group' has been removed.\")\n",
    "else:\n",
    "    print(\"Column 'group' not found in the dataset.\")\n",
    "\n",
    "# Save updated DataFrame to a new CSV file\n",
    "output_file = csv_file.replace(\".csv\", \"_updated.csv\")\n",
    "df_part4.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated file saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into MySQL database.\n"
     ]
    }
   ],
   "source": [
    "# Database connection details\n",
    "db_config = {\n",
    "    \"host\": \"mysqldchen.mysql.database.azure.com\",\n",
    "    \"user\": \"dchenAdmin\",\n",
    "    \"port\": 3306,\n",
    "    \"password\": \"507password!\",\n",
    "    \"database\": \"icpsr_03088\"\n",
    "}\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Generate table name\n",
    "table_name = \"adss_data_part4\"\n",
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0004\\\\03088-0004-Data_updated.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part4 = pd.read_csv(csv_file)\n",
    "\n",
    "# Get column names and data types\n",
    "columns = df_part4.columns\n",
    "data_types = df_part4.dtypes\n",
    "\n",
    "# Mapping pandas dtypes to MySQL column types\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"INT\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Define the table schema with caseid and facid as PRIMARY KEYS\n",
    "create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "for col, dtype in zip(columns, data_types):\n",
    "    mysql_type = dtype_mapping.get(str(dtype), \"TEXT\")  # Default to TEXT if unknown\n",
    "    create_table_query += f\"`{col}` {mysql_type}, \"\n",
    "\n",
    "# Define composite primary key (caseid, facid)\n",
    "create_table_query += \"PRIMARY KEY (`caseid`, `facid`));\"\n",
    "\n",
    "# Execute the table creation query\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "insert_query = f\"INSERT IGNORE INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(columns))})\"\n",
    "# iteratively adds rows to the table\n",
    "for _, row in df_part4.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully loaded into MySQL database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICPSR_03088  Alcohol and Drug Services Study (ADSS)\n",
    "\n",
    "Part 7 - Phase 3 - Follow-up Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'group' has been removed.\n",
      "Updated file saved as: C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0007\\03088-0007-Data_updated.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the data from tsv and remove column named 'group'\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0007\\\\03088-0007-Data.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part7 = pd.read_csv(csv_file)\n",
    "\n",
    "# Check if 'group' column exists before dropping\n",
    "if 'group' in df_part7.columns:\n",
    "    df_part7 = df_part7.drop(columns=['group'])\n",
    "    print(\"Column 'group' has been removed.\")\n",
    "else:\n",
    "    print(\"Column 'group' not found in the dataset.\")\n",
    "\n",
    "# Save updated DataFrame to a new CSV file\n",
    "output_file = csv_file.replace(\".csv\", \"_updated.csv\")\n",
    "df_part7.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated file saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into MySQL database.\n"
     ]
    }
   ],
   "source": [
    "# Database connection details\n",
    "db_config = {\n",
    "    \"host\": \"mysqldchen.mysql.database.azure.com\",\n",
    "    \"user\": \"dchenAdmin\",\n",
    "    \"port\": 3306,\n",
    "    \"password\": \"507password!\",\n",
    "    \"database\": \"icpsr_03088\"\n",
    "}\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Generate table name\n",
    "table_name = \"adss_data_part7\"\n",
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0007\\\\03088-0007-Data_updated.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part7 = pd.read_csv(csv_file)\n",
    "\n",
    "# Get column names and data types\n",
    "columns = df_part7.columns\n",
    "data_types = df_part7.dtypes\n",
    "\n",
    "# Mapping pandas dtypes to MySQL column types\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"INT\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Define the table schema with caseid and facid as PRIMARY KEYS\n",
    "create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "for col, dtype in zip(columns, data_types):\n",
    "    mysql_type = dtype_mapping.get(str(dtype), \"TEXT\")  # Default to TEXT if unknown\n",
    "    create_table_query += f\"`{col}` {mysql_type}, \"\n",
    "\n",
    "# Define composite primary key (caseid, facid)\n",
    "create_table_query += \"PRIMARY KEY (`caseid`, `facid`));\"\n",
    "\n",
    "# Execute the table creation query\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "insert_query = f\"INSERT IGNORE INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(columns))})\"\n",
    "# iteratively adds rows to the table\n",
    "for _, row in df_part7.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully loaded into MySQL database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "database: cdcyrbss   \n",
    "CDC - Youth Risk Behavior Surveillance System (YRBSS) \n",
    "\n",
    "creating schema, and importing data \n",
    "\n",
    "primary key = 'CASEID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\AppData\\Local\\Temp\\ipykernel_39376\\3460893445.py:21: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data successfully loaded into MySQL database.\n"
     ]
    }
   ],
   "source": [
    "# 3ector\n",
    "# import pandas as pd \n",
    "\n",
    "# Database connection details \n",
    "db_config = {\n",
    "    \"host\": \"mysqldchen.mysql.database.azure.com\",\n",
    "    \"user\": \"dchenAdmin\",\n",
    "    \"port\": 3306,\n",
    "    \"password\": \"507password!\",\n",
    "    \"database\": \"cdcyrbss\"\n",
    "}\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load CSV file\n",
    "csv_file = r\"C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\CDC\\XXHq.csv\"\n",
    "\n",
    "# Read CSV into pandas DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Convert column names to lowercase and remove spaces for compatibility\n",
    "df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Table name\n",
    "table_name = \"national2021\"\n",
    "\n",
    "# Get column names and data types\n",
    "columns = df.columns\n",
    "data_types = df.dtypes\n",
    "\n",
    "# Mapping pandas dtypes to MySQL column types\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"INT\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Define the table schema dynamically\n",
    "create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "column_definitions = []\n",
    "\n",
    "for col, dtype in zip(columns, data_types):\n",
    "    mysql_type = dtype_mapping.get(str(dtype), \"TEXT\")  # Default to TEXT if unknown\n",
    "    column_definitions.append(f\"`{col}` {mysql_type}\")\n",
    "\n",
    "# Add primary key (caseid)\n",
    "column_definitions.append(\"PRIMARY KEY (`record`)\")\n",
    "create_table_query += \", \".join(column_definitions) + \");\"\n",
    "\n",
    "# Execute the table creation query\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "insert_query = f\"INSERT IGNORE INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(columns))})\"\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\" Data successfully loaded into MySQL database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "database: dawn   \n",
    "Drug Abuse Warning Network (DAWN) \n",
    "\n",
    "creating schema, and importing data \n",
    "\n",
    "table name = 'data'\n",
    "\n",
    "primary key = 'CASEID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV file has been successfully converted to CSV and saved at: C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\Drug Abuse Warning Network (DAWN) ICPSR_34565\\DS0001\\34565-0001-Data.csv\n"
     ]
    }
   ],
   "source": [
    "# convert DAWN data TSV to CSV\n",
    "\n",
    "# Define the file paths \n",
    "tsv_file_path = r'C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\Drug Abuse Warning Network (DAWN) ICPSR_34565\\DS0001\\34565-0001-Data.tsv'\n",
    "csv_file_path = r'C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\Drug Abuse Warning Network (DAWN) ICPSR_34565\\DS0001\\34565-0001-Data.csv'\n",
    "\n",
    "# Open the TSV file for reading and the CSV file for writing\n",
    "with open(tsv_file_path, 'r', newline='', encoding='utf-8') as tsv_file, \\\n",
    "     open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    \n",
    "    # Create a TSV reader and a CSV writer\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "\n",
    "    # Iterate over each row in the TSV file and write it to the CSV file\n",
    "    for row in tsv_reader:\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(f\"TSV file has been successfully converted to CSV and saved at: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'record'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'record'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Insert into staging table\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     23\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124m        INSERT IGNORE INTO raw_data (record, caseid, drug_name, incident_type, age_group, location, year, processed_flag)\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m        VALUES (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, FALSE)\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m, (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecord\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaseid\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug_name\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincident_type\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage_group\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     28\u001b[0m conn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[0;32m     29\u001b[0m cursor\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'record'"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    \"host\": \"mysqldchen.mysql.database.azure.com\",\n",
    "    \"user\": \"dchenAdmin\",\n",
    "    \"port\": 3306,\n",
    "    \"password\": \"507password!\",\n",
    "    \"database\": \"dawn\"\n",
    "}\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load CSV file\n",
    "csv_file = r\"C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\Drug Abuse Warning Network (DAWN) ICPSR_34565\\DS0001\\34565-0001-Data.csv\"\n",
    "\n",
    "# Read only first 5000 rows from CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Adjust column names\n",
    "df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "table_name = \"data\"\n",
    "columns = df.columns\n",
    "data_types = df.dtypes\n",
    "\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"INT\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Define the table schema\n",
    "column_definitions = [\n",
    "    f\"`{col}` {dtype_mapping.get(str(dtype), 'TEXT')} {'PRIMARY KEY' if col == 'caseid' else ''}\"\n",
    "    for col, dtype in zip(columns, data_types)\n",
    "]\n",
    "\n",
    "# Create the partitioned table\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    {', '.join(column_definitions)}\n",
    ") PARTITION BY HASH(caseid) PARTITIONS 20;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "print(\"Table created successfully.\")\n",
    "\n",
    "# Prepare insert query\n",
    "insert_query = f\"\"\"\n",
    "INSERT IGNORE INTO {table_name} ({', '.join(columns)})\n",
    "VALUES ({', '.join(['%s'] * len(columns))})\n",
    "\"\"\"\n",
    "\n",
    "data = df.where(pd.notnull(df), None).values.tolist()\n",
    "\n",
    "cursor.executemany(insert_query, data)\n",
    "conn.commit()\n",
    "print(\"records inserted.\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Loading Data to Unified Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created new database: analytics_db\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "db_config = {\n",
    "    'host': 'mysqldchen.mysql.database.azure.com',\n",
    "    'user': 'dchenAdmin',\n",
    "    'password': '507password!',\n",
    "    'port': 3306\n",
    "}\n",
    "\n",
    "# Connect to MySQL server\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create new database 'analytics_db'\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS analytics_db;\")\n",
    "\n",
    "print(\"✅ Created new database: analytics_db\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "dawn_df = pd.read_csv(\"dawn_data.tsv\", sep=\"\\t\")\n",
    "yrbss_df = pd.read_csv(\"yrbss_data.csv\")\n",
    "nahdap_df = pd.read_csv(\"nahdap_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dawn_df.fillna(\"Unknown\", inplace=True)\n",
    "yrbss_df.fillna(\"Unknown\", inplace=True)\n",
    "nahdap_df.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrbss_df[\"substance_abuse_rate\"] = yrbss_df[\"students_using_drugs\"] / yrbss_df[\"total_students\"] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(\"\"\"\n",
    "CREATE TABLE dawn (\n",
    "    case_id INT PRIMARY KEY,\n",
    "    drug_name VARCHAR(255),\n",
    "    incident_type VARCHAR(255),\n",
    "    age_group VARCHAR(50),\n",
    "    location VARCHAR(255),\n",
    "    year INT\n",
    ");\n",
    "\n",
    "CREATE TABLE yrbss (\n",
    "    survey_id INT PRIMARY KEY,\n",
    "    state VARCHAR(255),\n",
    "    total_students INT,\n",
    "    students_using_drugs INT,\n",
    "    substance_abuse_rate FLOAT\n",
    ");\n",
    "\n",
    "CREATE TABLE nahdap (\n",
    "    record_id INT PRIMARY KEY,\n",
    "    facility_name VARCHAR(255),\n",
    "    treatment_capacity INT,\n",
    "    patients_served INT\n",
    ");\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"mysqldchen.mysql.database.azure.com\",\n",
    "    user=\"dchenAdmin\",\n",
    "    password=\"507password!\",\n",
    "    database=\"icpsr_03088\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Example: Insert DAWN Data\n",
    "for _, row in dawn_df.iterrows():\n",
    "    cursor.execute(\"INSERT INTO dawn (case_id, drug_name, incident_type, age_group, location, year) VALUES (%s, %s, %s, %s, %s, %s)\", \n",
    "                   (row['case_id'], row['drug_name'], row['incident_type'], row['age_group'], row['location'], row['year']))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation and Scheduling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the apache-airflow package\n",
    "# %pip install apache-airflow\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract():\n",
    "    print(\"Extracting data...\")\n",
    "\n",
    "def transform():\n",
    "    print(\"Transforming data...\")\n",
    "\n",
    "def load():\n",
    "    print(\"Loading data into database...\")\n",
    "\n",
    "dag = DAG(\n",
    "    'data_pipeline',\n",
    "    schedule_interval='@daily',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    ")\n",
    "\n",
    "task_extract = PythonOperator(task_id='extract', python_callable=extract, dag=dag)\n",
    "task_transform = PythonOperator(task_id='transform', python_callable=transform, dag=dag)\n",
    "task_load = PythonOperator(task_id='load', python_callable=load, dag=dag)\n",
    "\n",
    "task_extract >> task_transform >> task_load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ETL pipline using Airflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student: Darren Chen\n",
    "# ADS 507 \n",
    "# Assignment 5.1  - SQL Query Performance Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel python 3.10.12 \n",
    "# Install the mysql-connector-python package\n",
    "# %pip install mysql\n",
    "# %pip install mysql-connector-python\n",
    "\n",
    "import mysql.connector \n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Azure Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure SQL Server Connection \n",
    "conn = mysql.connector.connect(\n",
    "    host=\"mysqldchen.mysql.database.azure.com\",\n",
    "    user=\"dchenAdmin\",\n",
    "    port=\"3306\",\n",
    "    password=\"507password!\",\n",
    "    database=\"cdcyrbss\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_query function \n",
    "def run_query(query):\n",
    "    # try: (indent next for error handling)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    for row in results:\n",
    "        print(row)\n",
    "    # except mysql.connector.Error as err:\n",
    "    #     print(f\"Error: {err}\")\n",
    "    # finally:\n",
    "    #     if conn.is_connected():\n",
    "    #         cursor.close()\n",
    "    #         conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ADSS data to sql datab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL DATA: Alcohol and Drug Services Study (ADSS), 1996-1999: [United States] (ICPSR 3088)\n",
    "LINK: https://www.icpsr.umich.edu/web/NAHDAP/studies/3088/export\n",
    "DOWNLOAD DELIMITED FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert TSV to CSV\n",
    "\n",
    "# Define the file paths \n",
    "tsv_file_path = r'C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0003\\03088-0003-Data.tsv'\n",
    "csv_file_path = r'C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0003\\03088-0003-Data.csv'\n",
    "\n",
    "# Open the TSV file for reading and the CSV file for writing\n",
    "with open(tsv_file_path, 'r', newline='', encoding='utf-8') as tsv_file, \\\n",
    "     open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    \n",
    "    # Create a TSV reader and a CSV writer\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "\n",
    "    # Iterate over each row in the TSV file and write it to the CSV file\n",
    "    for row in tsv_reader:\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(f\"TSV file has been successfully converted to CSV and saved at: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert tsv file to csv for import to mysql database: \n",
    "\n",
    "in VSCode Terminal enter the following to convert to csv:  \n",
    ">Import-Csv -Delimiter \"`t\" -Path \"C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0004\\03088-0004-Data.tsv\" | Export-Csv -NoTypeInformation -Path \"C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0004\\03088-0004-Data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICPSR_03088  Alcohol and Drug Services Study (ADSS)\n",
    "\n",
    "Part 3 - Phase 2 - Main Incentive Abstract data table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'group' has been removed.\n",
      "Updated file saved as: C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0003\\03088-0003-Data_updated.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0003\\\\03088-0003-Data.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part3 = pd.read_csv(csv_file)\n",
    "\n",
    "# Check if 'group' column exists before dropping\n",
    "if 'group' in df_part3.columns:\n",
    "    df_part3 = df_part3.drop(columns=['group'])\n",
    "    print(\"Column 'group' has been removed.\")\n",
    "else:\n",
    "    print(\"Column 'group' not found in the dataset.\")\n",
    "\n",
    "# Save updated DataFrame to a new CSV file\n",
    "output_file = csv_file.replace(\".csv\", \"_updated.csv\")\n",
    "df_part3.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated file saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into MySQL database.\n"
     ]
    }
   ],
   "source": [
    "# import mysql.connector\n",
    "# import pandas as pd\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    \"host\": \"mysqldchen.mysql.database.azure.com\",\n",
    "    \"user\": \"dchenAdmin\",\n",
    "    \"port\": 3306,\n",
    "    \"password\": \"507password!\",\n",
    "    \"database\": \"icpsr_03088\"\n",
    "}\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Generate table name\n",
    "table_name = \"adss_data_part3\"\n",
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0003\\\\03088-0003-Data_updated.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part3 = pd.read_csv(csv_file)\n",
    "\n",
    "# Get column names and data types\n",
    "columns = df_part3.columns\n",
    "data_types = df_part3.dtypes\n",
    "\n",
    "# Mapping pandas dtypes to MySQL column types\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"INT\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Define the table schema with caseid and facid as PRIMARY KEYS\n",
    "create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "for col, dtype in zip(columns, data_types):\n",
    "    mysql_type = dtype_mapping.get(str(dtype), \"TEXT\")  # Default to TEXT if unknown\n",
    "    create_table_query += f\"`{col}` {mysql_type}, \"\n",
    "\n",
    "# Define composite primary key (caseid, facid)\n",
    "create_table_query += \"PRIMARY KEY (`caseid`, `facid`));\"\n",
    "\n",
    "# Execute the table creation query\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "insert_query = f\"INSERT IGNORE INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(columns))})\"\n",
    "# iteratively adds rows to the table\n",
    "for _, row in df_part3.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully loaded into MySQL database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICPSR_03088  Alcohol and Drug Services Study (ADSS)\n",
    "\n",
    "Part 4 - Phase 2 - In-Treatment Methadone Abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'group' has been removed.\n",
      "Updated file saved as: C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0004\\03088-0004-Data_updated.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0004\\\\03088-0004-Data.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part4 = pd.read_csv(csv_file)\n",
    "\n",
    "# Check if 'group' column exists before dropping\n",
    "if 'group' in df_part4.columns:\n",
    "    df_part4 = df_part4.drop(columns=['group'])\n",
    "    print(\"Column 'group' has been removed.\")\n",
    "else:\n",
    "    print(\"Column 'group' not found in the dataset.\")\n",
    "\n",
    "# Save updated DataFrame to a new CSV file\n",
    "output_file = csv_file.replace(\".csv\", \"_updated.csv\")\n",
    "df_part4.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated file saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into MySQL database.\n"
     ]
    }
   ],
   "source": [
    "# Database connection details\n",
    "db_config = {\n",
    "    \"host\": \"mysqldchen.mysql.database.azure.com\",\n",
    "    \"user\": \"dchenAdmin\",\n",
    "    \"port\": 3306,\n",
    "    \"password\": \"507password!\",\n",
    "    \"database\": \"icpsr_03088\"\n",
    "}\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Generate table name\n",
    "table_name = \"adss_data_part4\"\n",
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0004\\\\03088-0004-Data_updated.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part4 = pd.read_csv(csv_file)\n",
    "\n",
    "# Get column names and data types\n",
    "columns = df_part4.columns\n",
    "data_types = df_part4.dtypes\n",
    "\n",
    "# Mapping pandas dtypes to MySQL column types\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"INT\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Define the table schema with caseid and facid as PRIMARY KEYS\n",
    "create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "for col, dtype in zip(columns, data_types):\n",
    "    mysql_type = dtype_mapping.get(str(dtype), \"TEXT\")  # Default to TEXT if unknown\n",
    "    create_table_query += f\"`{col}` {mysql_type}, \"\n",
    "\n",
    "# Define composite primary key (caseid, facid)\n",
    "create_table_query += \"PRIMARY KEY (`caseid`, `facid`));\"\n",
    "\n",
    "# Execute the table creation query\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "insert_query = f\"INSERT IGNORE INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(columns))})\"\n",
    "# iteratively adds rows to the table\n",
    "for _, row in df_part4.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully loaded into MySQL database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICPSR_03088  Alcohol and Drug Services Study (ADSS)\n",
    "\n",
    "Part 7 - Phase 3 - Follow-up Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'group' has been removed.\n",
      "Updated file saved as: C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0007\\03088-0007-Data_updated.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the data from tsv and remove column named 'group'\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0007\\\\03088-0007-Data.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part7 = pd.read_csv(csv_file)\n",
    "\n",
    "# Check if 'group' column exists before dropping\n",
    "if 'group' in df_part7.columns:\n",
    "    df_part7 = df_part7.drop(columns=['group'])\n",
    "    print(\"Column 'group' has been removed.\")\n",
    "else:\n",
    "    print(\"Column 'group' not found in the dataset.\")\n",
    "\n",
    "# Save updated DataFrame to a new CSV file\n",
    "output_file = csv_file.replace(\".csv\", \"_updated.csv\")\n",
    "df_part7.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated file saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into MySQL database.\n"
     ]
    }
   ],
   "source": [
    "# Database connection details\n",
    "db_config = {\n",
    "    \"host\": \"mysqldchen.mysql.database.azure.com\",\n",
    "    \"user\": \"dchenAdmin\",\n",
    "    \"port\": 3306,\n",
    "    \"password\": \"507password!\",\n",
    "    \"database\": \"icpsr_03088\"\n",
    "}\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Generate table name\n",
    "table_name = \"adss_data_part7\"\n",
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0007\\\\03088-0007-Data_updated.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part7 = pd.read_csv(csv_file)\n",
    "\n",
    "# Get column names and data types\n",
    "columns = df_part7.columns\n",
    "data_types = df_part7.dtypes\n",
    "\n",
    "# Mapping pandas dtypes to MySQL column types\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"INT\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Define the table schema with caseid and facid as PRIMARY KEYS\n",
    "create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "for col, dtype in zip(columns, data_types):\n",
    "    mysql_type = dtype_mapping.get(str(dtype), \"TEXT\")  # Default to TEXT if unknown\n",
    "    create_table_query += f\"`{col}` {mysql_type}, \"\n",
    "\n",
    "# Define composite primary key (caseid, facid)\n",
    "create_table_query += \"PRIMARY KEY (`caseid`, `facid`));\"\n",
    "\n",
    "# Execute the table creation query\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "insert_query = f\"INSERT IGNORE INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(columns))})\"\n",
    "# iteratively adds rows to the table\n",
    "for _, row in df_part7.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully loaded into MySQL database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "database: cdcyrbss   \n",
    "CDC - Youth Risk Behavior Surveillance System (YRBSS) \n",
    "\n",
    "creating schema, and importing data \n",
    "\n",
    "primary key = 'CASEID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\AppData\\Local\\Temp\\ipykernel_39376\\3460893445.py:21: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data successfully loaded into MySQL database.\n"
     ]
    }
   ],
   "source": [
    "# 3ector\n",
    "# import pandas as pd \n",
    "\n",
    "# Database connection details \n",
    "db_config = {\n",
    "    \"host\": \"mysqldchen.mysql.database.azure.com\",\n",
    "    \"user\": \"dchenAdmin\",\n",
    "    \"port\": 3306,\n",
    "    \"password\": \"507password!\",\n",
    "    \"database\": \"cdcyrbss\"\n",
    "}\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load CSV file\n",
    "csv_file = r\"C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\CDC\\XXHq.csv\"\n",
    "\n",
    "# Read CSV into pandas DataFrame\n",
    "cdcyrbss_df = pd.read_csv(csv_file)\n",
    "\n",
    "# Convert column names to lowercase and remove spaces for compatibility\n",
    "cdcyrbss_df.columns = cdcyrbss_df.columns.str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Table name\n",
    "table_name = \"national2021\"\n",
    "\n",
    "# Get column names and data types\n",
    "columns = cdcyrbss_df.columns\n",
    "data_types = cdcyrbss_df.dtypes\n",
    "\n",
    "# Mapping pandas dtypes to MySQL column types\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"INT\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Define the table schema dynamically\n",
    "create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "column_definitions = []\n",
    "\n",
    "for col, dtype in zip(columns, data_types):\n",
    "    mysql_type = dtype_mapping.get(str(dtype), \"TEXT\")  # Default to TEXT if unknown\n",
    "    column_definitions.append(f\"`{col}` {mysql_type}\")\n",
    "\n",
    "# Add primary key (caseid)\n",
    "column_definitions.append(\"PRIMARY KEY (`record`)\")\n",
    "create_table_query += \", \".join(column_definitions) + \");\"\n",
    "\n",
    "# Execute the table creation query\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "insert_query = f\"INSERT IGNORE INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(columns))})\"\n",
    "\n",
    "for _, row in cdcyrbss_df.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\" Data successfully loaded into MySQL database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'group' has been removed.\n",
      "Updated file saved as: C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\ICPSR_03088\\DS0004\\03088-0004-Data_updated.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0004\\\\03088-0004-Data.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part4 = pd.read_csv(csv_file)\n",
    "\n",
    "# Check if 'group' column exists before dropping\n",
    "if 'group' in df_part4.columns:\n",
    "    df_part4 = df_part4.drop(columns=['group'])\n",
    "    print(\"Column 'group' has been removed.\")\n",
    "else:\n",
    "    print(\"Column 'group' not found in the dataset.\")\n",
    "\n",
    "# Save updated DataFrame to a new CSV file\n",
    "output_file = csv_file.replace(\".csv\", \"_updated.csv\")\n",
    "df_part4.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated file saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into MySQL database.\n"
     ]
    }
   ],
   "source": [
    "# Database connection details\n",
    "db_config = {\n",
    "    \"host\": \"mysqldchen.mysql.database.azure.com\",\n",
    "    \"user\": \"dchenAdmin\",\n",
    "    \"port\": 3306,\n",
    "    \"password\": \"507password!\",\n",
    "    \"database\": \"icpsr_03088\"\n",
    "}\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Generate table name\n",
    "table_name = \"adss_data_part4\"\n",
    "\n",
    "# Define file path\n",
    "csv_file = \"C:\\\\Users\\\\darre\\\\OneDrive\\\\Documents\\\\ADS\\\\ADS 507 Data Engineering\\\\data\\\\ICPSR_03088\\\\DS0004\\\\03088-0004-Data_updated.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_part4 = pd.read_csv(csv_file)\n",
    "\n",
    "# Get column names and data types\n",
    "columns = df_part4.columns\n",
    "data_types = df_part4.dtypes\n",
    "\n",
    "# Mapping pandas dtypes to MySQL column types\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"INT\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Define the table schema with caseid and facid as PRIMARY KEYS\n",
    "create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "for col, dtype in zip(columns, data_types):\n",
    "    mysql_type = dtype_mapping.get(str(dtype), \"TEXT\")  # Default to TEXT if unknown\n",
    "    create_table_query += f\"`{col}` {mysql_type}, \"\n",
    "\n",
    "# Define composite primary key (caseid, facid)\n",
    "create_table_query += \"PRIMARY KEY (`caseid`, `facid`));\"\n",
    "\n",
    "# Execute the table creation query\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "insert_query = f\"INSERT IGNORE INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(columns))})\"\n",
    "# iteratively adds rows to the table\n",
    "for _, row in df_part4.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully loaded into MySQL database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "database: dawn   \n",
    "Drug Abuse Warning Network (DAWN) \n",
    "\n",
    "creating schema, and importing data \n",
    "\n",
    "table name = 'data'\n",
    "\n",
    "primary key = 'CASEID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV file has been successfully converted to CSV and saved at: C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\Drug Abuse Warning Network (DAWN) ICPSR_34565\\DS0001\\34565-0001-Data.csv\n"
     ]
    }
   ],
   "source": [
    "# convert DAWN data TSV to CSV\n",
    "\n",
    "# Define the file paths \n",
    "tsv_file_path = r'C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\Drug Abuse Warning Network (DAWN) ICPSR_34565\\DS0001\\34565-0001-Data.tsv'\n",
    "csv_file_path = r'C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\Drug Abuse Warning Network (DAWN) ICPSR_34565\\DS0001\\34565-0001-Data.csv'\n",
    "\n",
    "# Open the TSV file for reading and the CSV file for writing\n",
    "with open(tsv_file_path, 'r', newline='', encoding='utf-8') as tsv_file, \\\n",
    "     open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    \n",
    "    # Create a TSV reader and a CSV writer\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "\n",
    "    # Iterate over each row in the TSV file and write it to the CSV file\n",
    "    for row in tsv_reader:\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(f\"TSV file has been successfully converted to CSV and saved at: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the table of the drug codes and drug labels into DAWN database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully imported into dawn.druglookup.\n"
     ]
    }
   ],
   "source": [
    "# Define file path\n",
    "file_path = r\"C:/Users/darre/OneDrive/Documents/ADS/ADS 507 Data Engineering/data/Drug Abuse Warning Network (DAWN) ICPSR_34565/drug_lookup.csv\"\n",
    "\n",
    "# Read CSV into DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create the table in the DAWN database if it doesn't exist\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dawn.druglookup (\n",
    "    Value INT PRIMARY KEY,\n",
    "    Drug_Label VARCHAR(255) NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "run_query(create_table_query)\n",
    "\n",
    "# Construct SQL INSERT statements\n",
    "values_string = \", \".join([f\"({row.Value}, '{row.Drug_Label}')\" for row in df.itertuples(index=False)])\n",
    "insert_query = f\"\"\"\n",
    "INSERT INTO dawn.druglookup (Value, Drug_Label)\n",
    "VALUES {values_string};\n",
    "\"\"\"\n",
    "\n",
    "# Execute batch insert\n",
    "run_query(insert_query)\n",
    "\n",
    "print(\"CSV file successfully imported into dawn.druglookup.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drug Abuse Warning Network (DAWN) Data load \n",
    "\n",
    "Due to large dataset size 229,212 records with 285 columns, use BULK INSERT.  \n",
    "\n",
    "1. Azure Created Storage Account (resource) and uploaded CSV file. \n",
    "2. \n",
    "\n",
    "\n",
    "Option 1: Use BULK INSERT (Recommended for Large Data)\n",
    "\n",
    "    Enable Bulk Insert:\n",
    "\n",
    "EXEC sp_configure 'show advanced options', 1;\n",
    "RECONFIGURE;\n",
    "EXEC sp_configure 'Ad Hoc Distributed Queries', 1;\n",
    "RECONFIGURE;\n",
    "\n",
    "Load the CSV from Azure Blob Storage:\n",
    "\n",
    "BULK INSERT er_data\n",
    "FROM 'https://ads507storage.blob.core.windows.net/container1/34565-0001-Data.csv'\n",
    "WITH (\n",
    "    DATA_SOURCE = 'MyAzureBlobStorage',\n",
    "    FORMAT = 'CSV',\n",
    "    FIRSTROW = 2,  -- Skip header\n",
    "    FIELDTERMINATOR = ',',\n",
    "    ROWTERMINATOR = '\\n',\n",
    "    TABLOCK\n",
    ");\n",
    "\n",
    "\n",
    "Create an External Data Source (One-Time Setup):\n",
    "\n",
    "CREATE DATABASE SCOPED CREDENTIAL MyAzureCredential\n",
    "WITH IDENTITY = 'SHARED ACCESS SIGNATURE',\n",
    "SECRET = 'your_sas_token';\n",
    "\n",
    "CREATE EXTERNAL DATA SOURCE MyAzureBlobStorage\n",
    "WITH (\n",
    "    TYPE = BLOB_STORAGE,\n",
    "    LOCATION = 'https://ads507storage.blob.core.windows.net/csv-uploads',\n",
    "    CREDENTIAL = MyAzureCredential\n",
    ");\n",
    "\n",
    "Now, BULK INSERT will work with your Blob Storage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Table created successfully.\n",
      "✅ Inserted 1000 rows...\n",
      "✅ Inserted 2000 rows...\n",
      "✅ Inserted 3000 rows...\n",
      "✅ Inserted 4000 rows...\n",
      "✅ Inserted 5000 rows...\n",
      "🎉 All records inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Database connection details\n",
    "db_config = {\n",
    "    \"host\": \"mysqldchen.mysql.database.azure.com\",\n",
    "    \"user\": \"dchenAdmin\",\n",
    "    \"port\": 3306,\n",
    "    \"password\": \"507password!\",\n",
    "    \"database\": \"dawn\"\n",
    "}\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load CSV file\n",
    "csv_file = r\"C:\\Users\\darre\\OneDrive\\Documents\\ADS\\ADS 507 Data Engineering\\data\\Drug Abuse Warning Network (DAWN) ICPSR_34565\\DS0001\\34565-0001-Data.csv\"\n",
    "\n",
    "# Read only first 5000 rows from CSV\n",
    "dawn_df = pd.read_csv(csv_file, nrows=5000)\n",
    "\n",
    "# Adjust column names\n",
    "dawn_df.columns = dawn_df.columns.str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "table_name = \"er_data\"\n",
    "\n",
    "columns = dawn_df.columns\n",
    "data_types = dawn_df.dtypes\n",
    "\n",
    "# Mapping pandas dtypes to MySQL\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"INT\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Define the table schema dynamically\n",
    "column_definitions = \",\\n\".join([\n",
    "    f\"`{col}` {dtype_mapping.get(str(dtype), 'TEXT')} {'PRIMARY KEY' if col == 'caseid' else ''}\".strip()\n",
    "    for col, dtype in zip(columns, data_types)\n",
    "])\n",
    "\n",
    "# ✅ FIX: Define the missing `CREATE TABLE` query\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    {column_definitions}\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute table creation\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "print(\"✅ Table created successfully.\")\n",
    "\n",
    "# Prepare insert query\n",
    "insert_query = f\"\"\"\n",
    "INSERT IGNORE INTO {table_name} ({', '.join(columns)})\n",
    "VALUES ({', '.join(['%s'] * len(columns))})\n",
    "\"\"\"\n",
    "\n",
    "# Convert NaNs to None for SQL compatibility\n",
    "data = dawn_df.where(pd.notnull(dawn_df), None).values.tolist()\n",
    "\n",
    "# ✅ Improved performance: Insert data in batches\n",
    "batch_size = 1000\n",
    "for i in range(0, len(data), batch_size):\n",
    "    cursor.executemany(insert_query, data[i:i + batch_size])\n",
    "    conn.commit()\n",
    "    print(f\"✅ Inserted {i + batch_size} rows...\")\n",
    "\n",
    "print(\"🎉 All records inserted successfully.\")\n",
    "\n",
    "# Close database connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Loading Data to Unified Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation and Scheduling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: apache-airflow in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (2.10.5)\n",
      "Requirement already satisfied: alembic<2.0,>=1.13.1 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.14.1)\n",
      "Requirement already satisfied: argcomplete>=1.10 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (3.5.3)\n",
      "Requirement already satisfied: asgiref>=2.3.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (3.8.1)\n",
      "Requirement already satisfied: attrs>=22.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (23.1.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (1.6.2)\n",
      "Requirement already satisfied: colorlog>=6.8.2 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (6.9.0)\n",
      "Requirement already satisfied: configupdater>=3.1.1 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (3.2)\n",
      "Requirement already satisfied: connexion<3.0,>=2.14.2 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from connexion[flask]<3.0,>=2.14.2->apache-airflow) (2.14.2)\n",
      "Requirement already satisfied: cron-descriptor>=1.2.24 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.4.5)\n",
      "Requirement already satisfied: croniter>=2.0.2 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (6.0.0)\n",
      "Requirement already satisfied: cryptography>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (42.0.2)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.2.18)\n",
      "Requirement already satisfied: dill>=0.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (0.3.7)\n",
      "Requirement already satisfied: flask-caching>=2.0.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (2.3.1)\n",
      "Requirement already satisfied: flask-session<0.6,>=0.4.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (0.5.0)\n",
      "Requirement already satisfied: flask-wtf>=1.1.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.2.2)\n",
      "Requirement already satisfied: flask<2.3,>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (2.2.5)\n",
      "Requirement already satisfied: fsspec>=2023.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (2023.10.0)\n",
      "Requirement already satisfied: google-re2>=1.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.1.20240702)\n",
      "Requirement already satisfied: gunicorn>=20.1.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (23.0.0)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (0.28.1)\n",
      "Requirement already satisfied: importlib_metadata>=6.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (7.0.1)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (2.0.1)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (4.19.2)\n",
      "Requirement already satisfied: lazy-object-proxy>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (1.6.0)\n",
      "Requirement already satisfied: linkify-it-py>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (2.0.0)\n",
      "Requirement already satisfied: lockfile>=0.12.2 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (0.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (2.2.0)\n",
      "Requirement already satisfied: markupsafe>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (2.1.3)\n",
      "Requirement already satisfied: marshmallow-oneofschema>=2.0.1 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (3.1.1)\n",
      "Requirement already satisfied: mdit-py-plugins>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (0.3.0)\n",
      "Requirement already satisfied: methodtools>=0.4.7 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (0.4.7)\n",
      "Requirement already satisfied: opentelemetry-api>=1.24.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp>=1.24.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.30.0)\n",
      "Requirement already satisfied: packaging>=23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (23.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (0.10.3)\n",
      "Requirement already satisfied: pendulum<4.0,>=2.1.2 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (3.0.0)\n",
      "Requirement already satisfied: pluggy>=1.5.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.5.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (5.9.0)\n",
      "Requirement already satisfied: pygments>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (2.15.1)\n",
      "Requirement already satisfied: pyjwt>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (2.4.0)\n",
      "Requirement already satisfied: python-daemon>=3.0.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (2.8.2)\n",
      "Requirement already satisfied: python-nvd3>=0.15.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (0.16.0)\n",
      "Requirement already satisfied: python-slugify>=5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (5.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (1.0.0)\n",
      "Requirement already satisfied: rfc3339-validator>=0.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (0.1.4)\n",
      "Requirement already satisfied: rich-argparse>=1.0.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.7.0)\n",
      "Requirement already satisfied: rich>=12.4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (13.3.5)\n",
      "Requirement already satisfied: setproctitle>=1.3.3 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.3.5)\n",
      "Requirement already satisfied: sqlalchemy<2.0,>=1.4.36 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.4.54)\n",
      "Requirement already satisfied: sqlalchemy-jsonfield>=1.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.0.2)\n",
      "Requirement already satisfied: tabulate>=0.7.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (0.9.0)\n",
      "Requirement already satisfied: tenacity!=8.2.0,>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (8.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (2.5.0)\n",
      "Requirement already satisfied: universal-pathlib!=0.2.4,>=0.2.2 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (0.2.6)\n",
      "Requirement already satisfied: werkzeug<3,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow) (2.2.3)\n",
      "Requirement already satisfied: apache-airflow-providers-common-compat in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.3.0)\n",
      "Requirement already satisfied: apache-airflow-providers-common-io in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.5.0)\n",
      "Requirement already satisfied: apache-airflow-providers-common-sql in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.21.0)\n",
      "Requirement already satisfied: apache-airflow-providers-fab>=1.0.2 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.5.3)\n",
      "Requirement already satisfied: apache-airflow-providers-ftp in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (3.12.0)\n",
      "Requirement already satisfied: apache-airflow-providers-http in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (5.0.0)\n",
      "Requirement already satisfied: apache-airflow-providers-imap in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (3.8.0)\n",
      "Requirement already satisfied: apache-airflow-providers-smtp in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (1.9.0)\n",
      "Requirement already satisfied: apache-airflow-providers-sqlite in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow) (4.0.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from alembic<2.0,>=1.13.1->apache-airflow) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\programdata\\anaconda3\\lib\\site-packages (from alembic<2.0,>=1.13.1->apache-airflow) (4.9.0)\n",
      "Requirement already satisfied: flask-appbuilder==4.5.3 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.5.3)\n",
      "Requirement already satisfied: flask-login>=0.6.2 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.6.3)\n",
      "Requirement already satisfied: jmespath>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow-providers-fab>=1.0.2->apache-airflow) (1.0.1)\n",
      "Requirement already satisfied: apispec<7,>=6.0.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apispec[yaml]<7,>=6.0.0->flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (6.8.1)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.4.6)\n",
      "Requirement already satisfied: click<9,>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (8.1.7)\n",
      "Requirement already satisfied: email-validator>=1.0.5 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.2.0)\n",
      "Requirement already satisfied: Flask-Babel<3,>=1 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.0.0)\n",
      "Requirement already satisfied: Flask-Limiter<4,>3 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.10.1)\n",
      "Requirement already satisfied: Flask-SQLAlchemy<3,>=2.4 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.5.1)\n",
      "Requirement already satisfied: Flask-JWT-Extended<5.0.0,>=4.0.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.7.1)\n",
      "Requirement already satisfied: marshmallow<4,>=3.18.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.26.1)\n",
      "Requirement already satisfied: marshmallow-sqlalchemy<0.29.0,>=0.22.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.28.2)\n",
      "Requirement already satisfied: prison<1.0.0,>=0.2.1 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.2.1)\n",
      "Requirement already satisfied: sqlalchemy-utils<1,>=0.32.21 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.41.2)\n",
      "Requirement already satisfied: WTForms<4 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.2.1)\n",
      "Requirement already satisfied: clickclick<21,>=1.2 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow) (20.10.2)\n",
      "Requirement already satisfied: PyYAML<7,>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow) (6.0.1)\n",
      "Requirement already satisfied: inflection<0.6,>=0.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow) (0.5.1)\n",
      "Requirement already satisfied: pytz>2021.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from croniter>=2.0.2->apache-airflow) (2023.3.post1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=41.0.0->apache-airflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from deprecated>=1.2.13->apache-airflow) (1.14.1)\n",
      "Requirement already satisfied: cachelib>=0.9.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from flask-caching>=2.0.0->apache-airflow) (0.13.0)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apache-airflow) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apache-airflow) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from httpx>=0.25.0->apache-airflow) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apache-airflow) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx>=0.25.0->apache-airflow) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib_metadata>=6.5->apache-airflow) (3.17.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->apache-airflow) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->apache-airflow) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->apache-airflow) (0.10.6)\n",
      "Requirement already satisfied: uc-micro-py in c:\\programdata\\anaconda3\\lib\\site-packages (from linkify-it-py>=2.0.0->apache-airflow) (1.0.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.1.0->apache-airflow) (0.1.0)\n",
      "Requirement already satisfied: wirerope>=0.4.7 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from methodtools>=0.4.7->apache-airflow) (1.0.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.30.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.30.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.30.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.68.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.30.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.30.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.30.0)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from opentelemetry-proto==1.30.0->opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (5.29.3)\n",
      "Requirement already satisfied: tzdata>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pendulum<4.0,>=2.1.2->apache-airflow) (2023.3)\n",
      "Requirement already satisfied: time-machine>=2.6.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from pendulum<4.0,>=2.1.2->apache-airflow) (2.16.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.0->apache-airflow) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-slugify>=5.0->apache-airflow) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->apache-airflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->apache-airflow) (2.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy<2.0,>=1.4.36->apache-airflow) (3.0.1)\n",
      "Requirement already satisfied: more-itertools>=9.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow-providers-common-sql->apache-airflow) (10.1.0)\n",
      "Requirement already satisfied: sqlparse>=0.5.1 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow-providers-common-sql->apache-airflow) (0.5.3)\n",
      "Requirement already satisfied: aiohttp!=3.11.0,>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from apache-airflow-providers-http->apache-airflow) (3.9.3)\n",
      "Requirement already satisfied: aiosqlite>=0.20.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from apache-airflow-providers-sqlite->apache-airflow) (0.21.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.9.3)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=41.0.0->apache-airflow) (2.21)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.25.0->apache-airflow) (1.3.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from email-validator>=1.0.5->flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.7.0)\n",
      "Requirement already satisfied: Babel>=2.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from Flask-Babel<3,>=1->flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.11.0)\n",
      "Requirement already satisfied: limits>=3.13 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from Flask-Limiter<4,>3->flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.0.1)\n",
      "Requirement already satisfied: ordered-set<5,>4 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from Flask-Limiter<4,>3->flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.1.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in c:\\users\\darre\\appdata\\roaming\\python\\python311\\site-packages (from opentelemetry-sdk~=1.30.0->opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (0.51b0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\AppData\\Roaming\\Python\\Python311\\site-packages\\airflow\\__init__.py:36: RuntimeWarning: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers. The work to add Windows support is tracked via https://github.com/apache/airflow/issues/10388, but it is not a high priority.\n",
      "  warnings.warn(\n",
      "OSError while attempting to symlink the latest log directory\n"
     ]
    }
   ],
   "source": [
    "# Install the apache-airflow package\n",
    "# %pip install apache-airflow\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract():\n",
    "    print(\"Extracting data...\")\n",
    "\n",
    "def transform():\n",
    "    print(\"Transforming data...\")\n",
    "\n",
    "def load():\n",
    "    print(\"Loading data into database...\")\n",
    "\n",
    "dag = DAG(\n",
    "    'data_pipeline',\n",
    "    schedule_interval='@daily',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    ")\n",
    "\n",
    "task_extract = PythonOperator(task_id='extract', python_callable=extract, dag=dag)\n",
    "task_transform = PythonOperator(task_id='transform', python_callable=transform, dag=dag)\n",
    "task_load = PythonOperator(task_id='load', python_callable=load, dag=dag)\n",
    "\n",
    "task_extract >> task_transform >> task_load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ETL pipline using Airflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
